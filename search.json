[
  {
    "objectID": "professional_blog.html",
    "href": "professional_blog.html",
    "title": "Professional Blog",
    "section": "",
    "text": "A Descent into March Madness\n\n\n\n\n\nCan a computer beat a human in basketball?\n\n\n\n\n\nMar 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSpaceship Titanic - Introduction to Bayesian Models - Part 3\n\n\n\n\n\nFighting back against the aliens using GAMs and ROC.\n\n\n\n\n\nMar 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSpaceship Titanic - Introduction to Bayesian Models - Part 2\n\n\n\n\n\nUsing a Stan model to improve over random guessing.\n\n\n\n\n\nMar 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSpaceship Titanic - Introduction to Bayesian Models - Part 1\n\n\n\n\n\nPredict which passengers are transported to an alternate dimension.\n\n\n\n\n\nMar 8, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "professional_blog/2024-03-10-spaceship-titanic-part-3/index.html#age-is-non-linear",
    "href": "professional_blog/2024-03-10-spaceship-titanic-part-3/index.html#age-is-non-linear",
    "title": "Spaceship Titanic - Introduction to Bayesian Models - Part 3",
    "section": "Age is Non-Linear",
    "text": "Age is Non-Linear\nThe first thing I want to address in our model is the Age variable.\nIncluding Age as a linear predictor probably captured some of the effects that it has, but in our univariate analysis we showed that the relationship Age has with Transported Status is not necessarily linear.\n\n\n\nIn fact, it’s hard to even visualize a continuous variable against a TRUE/FALSE response variable, and you might even be wondering what that blue line means, since it’s not the Age variable and it’s not the Transported variable.\nWhen I originally plotted this line in Part 1, I used a technique called loess which is a method for smoothing a relationship between two variables, for example Transportation ~ Age.\nMy idea for a more effective use of Age in the model is this:\n\nWhat if instead of using the Age variable directly, we applied some mapping from Age to a quantity close to the blue line in the graph, and then used that newly fitted value as our predictive variable instead of Age?\nThis should capture the relationship between Age and Transported more effectively than a linear relationship, with the drawback that the model coefficients aren’t as easily interpretable, and we run the risk of over-fitting. Ideally we should produce a somewhat generalized fit without jagged points to reduce the over-fitting risk.\nSimilar idea to the blue line: We can use a generalized additive model (GAM) to fit the univariate model Transportation ~ Age, and then replace the Age column with the fit from this model.\n\n\nFitting the GAM model.\nLet’s fit a GAM model between Transportation and Age using the mcgv package.\n\nlibrary(mgcv)\nage_model &lt;- gam(Transported ~ s(Age), data = imputed_sst_data, family = binomial(link = \"logit\"))\n\nage_grid &lt;- with(imputed_sst_data, data.frame(Age = seq(min(Age, na.rm = TRUE), max(Age, na.rm = TRUE), length.out = 100)))\nage_grid$Predicted &lt;- predict(age_model, newdata = age_grid, type = \"response\")\n\nggplot(age_grid, aes(x = Age, y = Predicted)) + \n  geom_line(color = \"#2C3E50\") +\n  geom_ribbon(aes(ymin = Predicted - 2*sqrt(Predicted * (1 - Predicted) / nrow(age_grid)),\n                  ymax = Predicted + 2*sqrt(Predicted * (1 - Predicted) / nrow(age_grid))),\n              fill = \"blue\", alpha = 0.2) +\n  rj_custom_theme()\n\n\n\n\n\n\n\n\nThat definitely looks line an interesting and non-linear relationship. I will append the predictions back to my original data set and call the new column Age_Pred.\n\n\nCode\nimputed_sst_data$Age_Pred &lt;- predict(age_model, imputed_sst_data, type = \"response\")"
  },
  {
    "objectID": "professional_blog/2024-03-10-spaceship-titanic-part-3/index.html#breaking-down-the-cabin-field",
    "href": "professional_blog/2024-03-10-spaceship-titanic-part-3/index.html#breaking-down-the-cabin-field",
    "title": "Spaceship Titanic - Introduction to Bayesian Models - Part 3",
    "section": "Breaking Down the Cabin Field",
    "text": "Breaking Down the Cabin Field\nThe other variable I wanted to add to our next model iteration was Cabin.\nThe cabin number where the passenger is staying. It takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n\n\nCode\nimputed_sst_data %&gt;% \n  select(Cabin) %&gt;% \n  head %&gt;% \n  rj_custom_table()\n\n\nCabinB/0/PF/0/SA/0/SA/0/SF/1/SF/0/P\n\n\nImplementing the information from the Cabin variable takes a little more complex data manipulation:\n\nSplit Cabin into three variables called deck, num, and side.\nImpute deck, num, or side values.\nUse the same GAM strategy on num also, which is to fit a GAM model Transportation ~ num so that we can capture the non-linear effect.\nWe can also create an indicator variable from the Cabin field. If multiple people are staying in the same Cabin then we can set a new variable called CabinMult to TRUE, and FALSE otherwise.\n\nI’ll do this before imputing any of the missing variables to avoid creating it using imputed data.\n\nLater, when I do the full model fit and submission to Kaggle, I’ll probably impute all missing variables at once (and also on the test set as well) instead of imputing one at a time.\n\n\ncabin_sst_data &lt;- imputed_sst_data %&gt;% \n  tidyr::separate(Cabin, into = c(\"deck\", \"num\", \"side\"), sep = \"/\", remove = FALSE, fill = \"right\") %&gt;% \n  mutate(deck = ifelse(deck == \"\", NA, deck)) %&gt;% \n  group_by(deck, num, side) %&gt;% \n  mutate(people_in_room = n(),\n         CabinMult = ifelse(people_in_room &gt; 1 & !is.na(deck), TRUE, FALSE)) %&gt;% \n  select(-c(Cabin, people_in_room)) %&gt;% \n  ungroup() %&gt;% \n  mutate(deck = factor(deck),\n         num = as.numeric(num),\n         side = factor(side))\n\ncabin_no_transport &lt;- cabin_sst_data %&gt;% select(-\"Transported\")\ncabin_transport &lt;- cabin_sst_data %&gt;% pull(\"Transported\")\n\nimputed_cabin &lt;- mice(cabin_no_transport, m = 1, printFlag = FALSE)\nimputed_cabin_data &lt;- complete(imputed_cabin, 1)\nimputed_cabin_data$Transported &lt;- cabin_transport\nimputed_cabin_data &lt;-\n  imputed_cabin_data %&gt;% \n  mutate(HomePlanet = factor(HomePlanet, levels = unique(HomePlanet)),\n         VIP = factor(VIP, levels = unique(VIP)),\n         deck = factor(deck, levels = unique(deck)),\n         side = factor(side, levels = unique(side)))\n\nnum_model &lt;- gam(Transported ~ s(num), data = imputed_cabin_data, family = binomial(link = \"logit\"))\nimputed_cabin_data$num_Pred &lt;- predict(num_model, imputed_cabin_data, type = \"response\")\n\nNow that we have the Cabin field broken out into deck, num, and side, we can create the design matrix, update the Stan code, and fit the model.\nNotice that currently we will be estimating a \\(\\beta\\) parameter for every deck on the ship, which is called a fixed effects model. But what if each deck is not that different from each other? If that is the case, we could set up a different structure for this effect, where each \\(\\beta_{deck}\\) was drawn from a common distribution, and the parameters of that distribution were estimated instead of the \\(\\beta_{deck}\\) themselves?\nThat describes a mixed-effects model, and it might be a better fit in the future when I have time to properly implement it in Stan. The benefits of this approach would be we only need to estimate one effect \\(\\beta\\) and standard deviation \\(\\sigma\\), instead of estimating a separate \\(\\beta\\) for every deck on the ship. It would also mean that for the decks where we don’t have a lot of data, more natural credibility would have to be given to the overall mean since the prior and posterior have to be close to each other when you don’t have much data.\n\n\nCode\nimputed_cabin_data %&gt;% \n  model.matrix(`Transported` ~ `Age_Pred` + `VIP` + `HomePlanet` + deck + num + side + CabinMult + num_Pred, data = .) %&gt;% \n  head() %&gt;% \n  as.data.frame() %&gt;% \n  rj_custom_table()\n\n\n(Intercept)Age_PredVIPTRUEHomePlanetEarthHomePlanetMarsdeckFdeckAdeckGdeckEdeckDdeckCdeckTnumsideSCabinMultTRUEnum_Pred10.485619600000000000000.505719810.468056601010000000100.505719810.487814410001000000110.505719810.475639200001000000110.505719810.517521101010000001100.506229710.488716601010000000000.5057198\n\n\nI have shown a sample of the latest design matrix above, so finally, it’s time to fit the model."
  },
  {
    "objectID": "professional_blog/2024-03-10-spaceship-titanic-part-3/index.html#second-model-run",
    "href": "professional_blog/2024-03-10-spaceship-titanic-part-3/index.html#second-model-run",
    "title": "Spaceship Titanic - Introduction to Bayesian Models - Part 3",
    "section": "Second Model Run",
    "text": "Second Model Run\nTo summarize, we added the following parameters to the second iteration of the model:\n\nA GAM fit to the Age variable.\nSplit Cabin into deck, num, and side.\nA GAM fit to the num variable.\nA variable called CabinMult that is true if multiple people shared the same cabin (that we know about.)\n\nLet’s go ahead and fit the Stan model using the exact same Stan code as last time without modifications. This is definitely a benefit of setting model structure using a design matrix and response vector structure, since they can easily be extended to have any number of observations and predictors.\n\ncabin_design_matrix &lt;- \n  imputed_cabin_data %&gt;% \n  model.matrix(`Transported` ~ `Age_Pred` + `VIP` + `HomePlanet` + deck + num + side + CabinMult + num_Pred, data = .)\n\ncabin_response &lt;- as.integer(imputed_cabin_data$Transported)\n\nif (!file.exists(\"../../assets/models/sst_model_2.rds\")){\n  sst_fit_2 &lt;- \n    stan(file = \"../../assets/stan/spaceship_titanic_2.stan\",\n         data = list(N = nrow(cabin_design_matrix),\n                     K = ncol(cabin_design_matrix),\n                     X = cabin_design_matrix,\n                     y = cabin_response),\n         chains = 2,\n         iter = 2000)\n  save(sst_fit_2, file = \"../../assets/models/sst_model_2.rds\")\n  } else {\n    load(\"../../assets/models/sst_model_2.rds\")\n  }"
  },
  {
    "objectID": "professional_blog/2024-03-10-spaceship-titanic-part-3/index.html#roc---we-have-two-models-which-is-better",
    "href": "professional_blog/2024-03-10-spaceship-titanic-part-3/index.html#roc---we-have-two-models-which-is-better",
    "title": "Spaceship Titanic - Introduction to Bayesian Models - Part 3",
    "section": "ROC - We have two models, which is better?",
    "text": "ROC - We have two models, which is better?\nLet’s see what the accuracy of this new model is compared to the first one using another confusion matrix.\n\ny_pred_means &lt;- apply(rstan::extract(sst_fit_2, \"y_pred\")$y_pred, 2, mean)\ny_pred_factor &lt;- factor(ifelse(y_pred_means &gt; 0.5, TRUE, FALSE))\ncabin_response_factor &lt;- factor(ifelse(cabin_response == 1, TRUE, FALSE))\n\nconfusionMatrix(y_pred_factor, cabin_response_factor)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction FALSE TRUE\n     FALSE  2871 1649\n     TRUE   1444 2729\n                                         \n               Accuracy : 0.6442         \n                 95% CI : (0.634, 0.6543)\n    No Information Rate : 0.5036         \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      \n                                         \n                  Kappa : 0.2886         \n                                         \n Mcnemar's Test P-Value : 0.0002444      \n                                         \n            Sensitivity : 0.6654         \n            Specificity : 0.6233         \n         Pos Pred Value : 0.6352         \n         Neg Pred Value : 0.6540         \n             Prevalence : 0.4964         \n         Detection Rate : 0.3303         \n   Detection Prevalence : 0.5200         \n      Balanced Accuracy : 0.6443         \n                                         \n       'Positive' Class : FALSE          \n                                         \n\n\nNot bad, we’re up to over 64% accuracy!\nFinally, we will take a look at the ROC curve, which looks at how the model performs by letting the TRUE/FALSE threshold vary from 0% to 100%.\nThe more area that is under this curve (which is a statistic called AUC, or area under the curve), the better the model performs at classification.\n\nlibrary(pROC)\n\nroc1 &lt;- roc(cabin_response_factor, apply(rstan::extract(sst_fit_1, \"y_pred\")$y_pred, 2, mean))\nroc2 &lt;- roc(cabin_response_factor, apply(rstan::extract(sst_fit_2, \"y_pred\")$y_pred, 2, mean))\n\nauc1 &lt;- auc(roc1)\nauc2 &lt;- auc(roc2)\n\nroc1_df &lt;- data.frame(tpr=roc1$sensitivities, fpr= 1 - roc1$specificities, model='Model 1')\nroc2_df &lt;- data.frame(tpr=roc2$sensitivities, fpr= 1 - roc2$specificities, model='Model 2')\n\n# Combine the data frames\nroc_df &lt;- rbind(roc1_df, roc2_df)\n\n# Plot with ggplot\nggplot(roc_df, aes(x=fpr, y=tpr, color=model)) + \n  geom_line() + \n  geom_abline(linetype=\"dashed\") +\n  scale_color_manual(values=c(\"Model 1\"=\"red\", \"Model 2\"=\"blue\")) +\n  labs(x=\"False Positive Rate\", y=\"True Positive Rate\", title=\"ROC Curve Comparison\") +\n  annotate(\"text\", x=0.6, y=0.25, label=paste(\"Model 1 AUC =\", round(auc1, 3)), color=\"red\", size = 8) +\n  annotate(\"text\", x=0.6, y=0.15, label=paste(\"Model 2 AUC =\", round(auc2, 3)), color=\"blue\", size = 8) +\n  rj_custom_theme()\n\n\n\n\n\n\n\n\nWe can see from the ROC Curve that Model 2 has a higher True Positive rate for every possible threshold (which is denoted by moving from left to right on the x-axis) and has a larger AUC, meaning it should perform better at classification than Model 1.\nI would hope that was the case, since I put a lot of effort into improving it. 😉"
  },
  {
    "objectID": "professional_blog/2024-03-10-spaceship-titanic-part-3/index.html#summary-part-4-preview",
    "href": "professional_blog/2024-03-10-spaceship-titanic-part-3/index.html#summary-part-4-preview",
    "title": "Spaceship Titanic - Introduction to Bayesian Models - Part 3",
    "section": "Summary + Part 4 Preview",
    "text": "Summary + Part 4 Preview\nPhew, that was a lot of content to get through! Thanks for sticking through it - I probably won’t try to cram much into Part 4 so that we can do a deeper dive on how the model is really performing so far.\nThanks again, and please subscribe to the blog if you haven’t already. A lot of effort and research goes into creating this type of material, and it will make me glad if people actually subscribed and read what I enjoy writing about.\nIn Part 4, I will explore one or two more predictive variables from our data, setting priors on variables for more efficient model fitting, residual plots, and possibly leave-one-out cross-validation if my computer decides to cooperate with that package.\n\nReturn to the top!"
  },
  {
    "objectID": "professional_blog/2024-03-08-spaceship-titanic-part-1/index.html#load-the-data",
    "href": "professional_blog/2024-03-08-spaceship-titanic-part-1/index.html#load-the-data",
    "title": "Spaceship Titanic - Introduction to Bayesian Models - Part 1",
    "section": "Load the Data",
    "text": "Load the Data\nThe first steps in any data analysis done in R will be loading the libraries you want to use and reading in the data to a data.frame or a data.table object. I load the rstan package because I will be writing the main part of the model in Stan, and set the options to use all of the cores on my computer and not recompile the model every time I run it.\n\n\nCode\nlibrary(rstan)\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(plotly)\nlibrary(mgcv)\nlibrary(showtext)\nlibrary(flextable)\n\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\nshowtext_auto()\n\nsst_data &lt;- data.table::fread(\"../../assets/data/train.csv\")\n\n\nFor reference, I will show the first few rows of the data set. For the sake of keeping the post shorter, you can refer to the Kaggle link for the definitions of each of the data fields.\n\n\nCode\nhead(sst_data) %&gt;% \nrj_custom_table()\n\n\nPassengerIdHomePlanetCryoSleepCabinDestinationAgeVIPRoomServiceFoodCourtShoppingMallSpaVRDeckNameTransported0001_01EuropaFALSEB/0/PTRAPPIST-1e39FALSE00000Maham OfracculyFALSE0002_01EarthFALSEF/0/STRAPPIST-1e24FALSE10992554944Juanna VinesTRUE0003_01EuropaFALSEA/0/STRAPPIST-1e58TRUE4335760671549Altark SusentFALSE0003_02EuropaFALSEA/0/STRAPPIST-1e33FALSE012833713329193Solam SusentFALSE0004_01EarthFALSEF/1/STRAPPIST-1e16FALSE303701515652Willy SantantinesTRUE0005_01EarthFALSEF/0/PPSO J318.5-2244FALSE048302910Sandie HinetthewsTRUE"
  },
  {
    "objectID": "professional_blog/2024-03-08-spaceship-titanic-part-1/index.html#modeling-approach",
    "href": "professional_blog/2024-03-08-spaceship-titanic-part-1/index.html#modeling-approach",
    "title": "Spaceship Titanic - Introduction to Bayesian Models - Part 1",
    "section": "Modeling Approach",
    "text": "Modeling Approach\nAfter looking at the predictive fields available in this data set, the first thing I try to do is come up with a plan to approach the modeling structure.\nI have a couple of initial thoughts here:\n\nWe are trying to predict the Transported field which is represented as either TRUE or FALSE in our data.\nThis leads me to believe that logistic regression is probably the best and simplest modelling approach.\nIn statistical terms, this means we want a model with the following structure: \\[\n\\log\\left(\\frac{\\pi}{1-\\pi}\\right) = \\alpha + \\beta X\n\\]\nThis transformation (often called the logit function or log-odds) is useful, because it is a function that maps values between 0 and 1, like probabilities, to real numbers.\nIn this model, \\(\\pi\\) will represent our response (Probability % of being Transported) and the \\(\\beta\\) will represent our predictive variables, so let’s explore some of the possibilities for \\(\\beta\\)."
  },
  {
    "objectID": "professional_blog/2024-03-08-spaceship-titanic-part-1/index.html#exploring-vip-status-age-and-home-planet-variables",
    "href": "professional_blog/2024-03-08-spaceship-titanic-part-1/index.html#exploring-vip-status-age-and-home-planet-variables",
    "title": "Spaceship Titanic - Introduction to Bayesian Models - Part 1",
    "section": "Exploring VIP Status, Age, and Home Planet variables",
    "text": "Exploring VIP Status, Age, and Home Planet variables\nI want to start with a simple working model and build from there, so I’ll start by just examining the relationship between a few of the explanatory variables and the Transported response variable.\nFirst of all, let’s see how many people were transported in total.\n\n\nCode\nsst_data %&gt;% \n  count(Transported) %&gt;% \n  mutate(Proportion = scales::percent_format()(n / sum(n))) %&gt;% \n  rj_custom_table()\n\n\nTransportednProportionFALSE431549.64%TRUE437850.36%\n\n\nThat looks like a fairly even split, which is good. It could be harder to model a huge imbalance of response classes, especially if you don’t have a lot of observations in the first place.\nI’ve found visual data analysis can be useful to see which of our variables might provide some information that explains a difference in categorizing who was transported and who was not transported.\n\nAge\n\n\nCode\nggplot(sst_data, aes(x = Age, y = as.numeric(Transported))) +\n  geom_point(alpha = 0.1) +\n  geom_smooth(method = \"loess\", formula = y ~ x, span = 0.5, se = FALSE, color = \"blue\") +\n  labs(title = \"Relationship between Age and Transported Status\",\n       x = \"Age\",\n       y = \"Probability of Being Transported\") +\n  rj_custom_theme()\n\n\n\n\n\n\n\n\n\n\n\nThere are 179 missing Age values in the dataset (which we will have to handle later.)\nIt looks like passengers from Age 0 to about Age 17 are much more likely to be transported, and then the effect kind of levels off after that.\n\n\nVIP Status\n\n\nCode\nggplot(sst_data, aes(x = VIP, fill = Transported)) + \n  geom_bar(position = \"fill\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(title = \"Proportion of Transported by VIP Status\", x = \"VIP Status\", y = \"Proportion Transported\") +\n  rj_custom_theme()\n\n\n\n\n\n\n\n\n\n\n\nVIPs were transported at a 38.2% rate, while non-VIPs were transported at a 50.6% rate.\n\n\nHome Planet\n\n\nCode\nggplot(sst_data, aes(x = HomePlanet, fill = Transported)) + \n  geom_bar(position = \"fill\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(title = \"Proportion of Transported by Home Planet\", x = \"Home Planet\", y = \"Proportion Transported\") +\n  rj_custom_theme()\n\n\n\n\n\n\n\n\n\n\n\nIt appears there are three possible Home Planets: Earth, Europa, and Mars.\nThey all have different Transportation proportions which makes it a great candidate for a categorical predictive variable.\nThe aliens really seem to have taken a liking to residents from Europa.\n\n\nSummary\nTo summarize, it looks like a non-VIP from Europa that is between Age 0 and 17 has the highest risk factors for being abducted.\n\nIt’s good to do sense checks on your exploration is saying, so let’s see if we can take a slice of the data (like a population that has two out of any three of these risk factors) and see how strong the effects we discovered are.\n\n\n\nCode\nsst_data %&gt;% \n  select(`VIP`, `Age`, `HomePlanet`, `Transported`) %&gt;% \n  filter((`VIP` == FALSE & `Age` &lt; 18) | (`VIP` == FALSE & `HomePlanet` == \"Europa\") | (`Age` &lt; 18 & `HomePlanet` == \"Europa\")) %&gt;% \n  count(Transported) %&gt;% \n  rj_custom_table()\n\n\nTransportednFALSE1180TRUE2119\n\n\nIt definitely seems like we have some potential predictive power if we use these variables since we were able to pull out twice as many TRUE as FALSE using this naive approach (not to be confused with a Naïve Bayes Classifier which relies on a similar concept.)\nIn the next post I will explore how to take these three insights and formally build them into a probability model using Stan."
  },
  {
    "objectID": "professional_blog/2024-03-08-spaceship-titanic-part-1/index.html#part-2-preview-building-the-model",
    "href": "professional_blog/2024-03-08-spaceship-titanic-part-1/index.html#part-2-preview-building-the-model",
    "title": "Spaceship Titanic - Introduction to Bayesian Models - Part 1",
    "section": "Part 2 Preview: Building the Model",
    "text": "Part 2 Preview: Building the Model\nThat’s enough data exploration content for one blog post - thanks for reading and my goal going forward will be to create short and digestible posts that are easy to casually read and understand without investing too much of your time.\nKeep these insights that I just discovered in mind, since those will be used in Part 2 to describe a Bayesian probability model on the odds our poor travelers were subjected to alien transportation.\nUPDATE: Part 2 is out! You can read it here.\n\nReturn to the top!"
  },
  {
    "objectID": "personal_blog.html",
    "href": "personal_blog.html",
    "title": "Personal Blog",
    "section": "",
    "text": "On this page I will talk about random personal topics that are interesting to me.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "music_review.html",
    "href": "music_review.html",
    "title": "Music Reviews",
    "section": "",
    "text": "Reserving this page for when I want to do some music reviews."
  },
  {
    "objectID": "food_review.html",
    "href": "food_review.html",
    "title": "Food Reviews",
    "section": "",
    "text": "On this page I will review restaurants and talk about the wide variety of food that is available in Chicago (and occasionally other cities!)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSawada Coffee\n\n\nLion’s Mane, sanshō, orange peel, plus a massive cookie.\n\n\n\n\n\n\nMar 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGalit (⭐)\n\n\nI ventured out to the Michelin Star restaurant named Galit in Lincoln Park to give chef Zach Engel’s take on Middle-Eastern cuisine a try.\n\n\n\n\n\n\nMar 7, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "food_review/2024-03-07-galit/index.html",
    "href": "food_review/2024-03-07-galit/index.html",
    "title": "Galit (⭐)",
    "section": "",
    "text": "We visited Galit, a Middle Eastern restaurant in the middle of the Lincoln Park neighborhood of Chicago last week.\nImmediately upon sitting down, my first two thoughts were how busy the restaurant was, and how you could see directly into the busy kitchen. Since we had never been here, we ordered the “4-course choose your own adventure” and the wine accompaniment. To be honest, I have not tried many of these foods and spices before, so I wasn’t quite sure what anything was going to taste like.\n\n\n\nTo start things off, we had the masabacha hummus, labneh, sweet potato, ezme, and pickles.\nMy first thought when these plates came out were: “Wow, look at the size of that pita bread!” We were quite hungry since our reservations were for 8:45pm, so everything immediately looked very appetizing.\nI was most interested in trying the sweet potato, the dish closest to me, so I tore a piece of the pita bread and ate it together with the sweet potato. It tasted fairly normal but tasty, and so did the pickles. I really like pickles and don’t get the opportunity enough to try pickled vegetables other than cucumbers.\nThe small dish with labneh was what I tried next, and with the pita it tasted kind of similar to butter on a roll but flavored a little more sweet and sour than butter.\nWhitney liked the ezme on the far side of the table more than I did, but she loves tomato dishes more than me. It didn’t have a strong tomato flavor though and with the other spices it balanced out the other dishes pretty well.\nFinally, we had the masabacha hummus in the middle which was delicious. I don’t normally eat hummus since I haven’t been sure if I liked it or not in the past, but this dish made me realize maybe I would like some types of hummus in my diet. Chickpeas are very nutritious and the consistency of the masabacha went perfectly with the remainder of my pita.\n\nThe wine pairing for this meal was Keush, ‘Ultra’ Blanc de Noir Brut Nature from Armenia. I’m not really an expert when it comes to wine tasting so I can’t really comment too much on any of the specific flavors from the wines I had but this one was memorable for being light and bubbly, almost like a champagne.\n\n\n\nNext, for mezze (appetizers), we ordered the grape leaves and carrots.\nMy initial impression was that I really don’t know what either of these foods is going to taste like, but the grape leaves immediately looked more appetizing so I grabbed one, dipped it in the yellow saffron sauce and gave it a try.\nTo my palate, it was very bitter initially. It tasted very healthy and full of nutrients though, and although I don’t know what grape leaves taste like normally they seemed to me to be very fresh and authentic. I split the third one with Whitney after trying the carrots and that other half tasted much less bitter, especially after dipped in more of the saffron sauce.\nThe carrots had an overwhelming flavor, maybe the piece I initially got had a ton of duqqa, which has a variety of warm spices in it. That is definitely not a flavor that is common in American cuisine, so in the moment I was really struggling to come up with a term on how I could describe the flavor of the carrots. The spices were very aromatic, warm, and earthy. There was so much covering the carrots and the flavor was so strong that I had to leave some spices on the plate, but overall a very tasty pair of appetizers.\n\nThe wine pairing for this meal was Yarden ‘Golan Heights’ Chardonnay Galilee from Israel.\n\n\n\nFor our main course, I ordered the lamb basteeya and Whitney got the mushroom schnitzel. She just went vegetarian in January 2024 and this was the only vegetarian option on the menu, so we did not share these dishes with each other, other than when I tried one bite of the mushroom.\nThe lamb was delicious, I regret not trying more of the sauces on my plate but I was definitely feeling the effects of three glasses of wine after the waitress had brought us a glass of Chateau Musar Gaston Hochar from Lebanon to enjoy with our entree.\n\nThe lamb was tender and tasty, and it kind of reminded me of a pot pie, except sweeter. The powdered sugar (at least that’s what I think it was - it was very sweet) topping was a break from the slight bitterness of the carrots and grape leaves of the previous meal. I tried a bite of the mushroom schnitzel. The outer coating of those was fried to a nice and crispy consistency, although the mushroom was a little more sour than I was expecting.\nI know that Lion’s Mane mushroom is supposed to be a natural mushroom good for boosting memory though. I’ve only ever really tried it through supplements. There’s a really cool coffee shop in the West Loop called Sawada Coffee that gives you the option to add Lion’s Mane mushroom powder to your coffee and it’s delicious - maybe that will be the next review. Stay tuned!\n\n\n\nFor our dessert, we got the olive oil cake. I wanted to try the phyllo pie - maybe next time! The crispy meringue was interesting, obviously very light and like biting into a cloud. The cake was definitely sweeter than I was expecting, I don’t know why I expected a much stronger flavor of olive oil.\n\nThe final wine pairing was a special one, it was a wine that has been brewed in the same method since biblical times which involves adding new barrels of wine to older barrels that are never fully emptied. The result was a very sweet and deliciously aged dessert wine called Keo St. John Commandaria from Cypress that was very enjoyable to drink when paired with the olive oil cake.\n\n\n\n\nOverall score: 4.6 out of 5.0 stars\nWould definitely go back here if I ever have the opportunity! Thanks for reading, and shalom y’all!"
  },
  {
    "objectID": "food_review/2024-03-07-galit/index.html#hummus-salatim",
    "href": "food_review/2024-03-07-galit/index.html#hummus-salatim",
    "title": "Galit (⭐)",
    "section": "",
    "text": "To start things off, we had the masabacha hummus, labneh, sweet potato, ezme, and pickles.\nMy first thought when these plates came out were: “Wow, look at the size of that pita bread!” We were quite hungry since our reservations were for 8:45pm, so everything immediately looked very appetizing.\nI was most interested in trying the sweet potato, the dish closest to me, so I tore a piece of the pita bread and ate it together with the sweet potato. It tasted fairly normal but tasty, and so did the pickles. I really like pickles and don’t get the opportunity enough to try pickled vegetables other than cucumbers.\nThe small dish with labneh was what I tried next, and with the pita it tasted kind of similar to butter on a roll but flavored a little more sweet and sour than butter.\nWhitney liked the ezme on the far side of the table more than I did, but she loves tomato dishes more than me. It didn’t have a strong tomato flavor though and with the other spices it balanced out the other dishes pretty well.\nFinally, we had the masabacha hummus in the middle which was delicious. I don’t normally eat hummus since I haven’t been sure if I liked it or not in the past, but this dish made me realize maybe I would like some types of hummus in my diet. Chickpeas are very nutritious and the consistency of the masabacha went perfectly with the remainder of my pita.\n\nThe wine pairing for this meal was Keush, ‘Ultra’ Blanc de Noir Brut Nature from Armenia. I’m not really an expert when it comes to wine tasting so I can’t really comment too much on any of the specific flavors from the wines I had but this one was memorable for being light and bubbly, almost like a champagne."
  },
  {
    "objectID": "food_review/2024-03-07-galit/index.html#mezze",
    "href": "food_review/2024-03-07-galit/index.html#mezze",
    "title": "Galit (⭐)",
    "section": "",
    "text": "Next, for mezze (appetizers), we ordered the grape leaves and carrots.\nMy initial impression was that I really don’t know what either of these foods is going to taste like, but the grape leaves immediately looked more appetizing so I grabbed one, dipped it in the yellow saffron sauce and gave it a try.\nTo my palate, it was very bitter initially. It tasted very healthy and full of nutrients though, and although I don’t know what grape leaves taste like normally they seemed to me to be very fresh and authentic. I split the third one with Whitney after trying the carrots and that other half tasted much less bitter, especially after dipped in more of the saffron sauce.\nThe carrots had an overwhelming flavor, maybe the piece I initially got had a ton of duqqa, which has a variety of warm spices in it. That is definitely not a flavor that is common in American cuisine, so in the moment I was really struggling to come up with a term on how I could describe the flavor of the carrots. The spices were very aromatic, warm, and earthy. There was so much covering the carrots and the flavor was so strong that I had to leave some spices on the plate, but overall a very tasty pair of appetizers.\n\nThe wine pairing for this meal was Yarden ‘Golan Heights’ Chardonnay Galilee from Israel."
  },
  {
    "objectID": "food_review/2024-03-07-galit/index.html#over-coal",
    "href": "food_review/2024-03-07-galit/index.html#over-coal",
    "title": "Galit (⭐)",
    "section": "",
    "text": "For our main course, I ordered the lamb basteeya and Whitney got the mushroom schnitzel. She just went vegetarian in January 2024 and this was the only vegetarian option on the menu, so we did not share these dishes with each other, other than when I tried one bite of the mushroom.\nThe lamb was delicious, I regret not trying more of the sauces on my plate but I was definitely feeling the effects of three glasses of wine after the waitress had brought us a glass of Chateau Musar Gaston Hochar from Lebanon to enjoy with our entree.\n\nThe lamb was tender and tasty, and it kind of reminded me of a pot pie, except sweeter. The powdered sugar (at least that’s what I think it was - it was very sweet) topping was a break from the slight bitterness of the carrots and grape leaves of the previous meal. I tried a bite of the mushroom schnitzel. The outer coating of those was fried to a nice and crispy consistency, although the mushroom was a little more sour than I was expecting.\nI know that Lion’s Mane mushroom is supposed to be a natural mushroom good for boosting memory though. I’ve only ever really tried it through supplements. There’s a really cool coffee shop in the West Loop called Sawada Coffee that gives you the option to add Lion’s Mane mushroom powder to your coffee and it’s delicious - maybe that will be the next review. Stay tuned!"
  },
  {
    "objectID": "food_review/2024-03-07-galit/index.html#dessert",
    "href": "food_review/2024-03-07-galit/index.html#dessert",
    "title": "Galit (⭐)",
    "section": "",
    "text": "For our dessert, we got the olive oil cake. I wanted to try the phyllo pie - maybe next time! The crispy meringue was interesting, obviously very light and like biting into a cloud. The cake was definitely sweeter than I was expecting, I don’t know why I expected a much stronger flavor of olive oil.\n\nThe final wine pairing was a special one, it was a wine that has been brewed in the same method since biblical times which involves adding new barrels of wine to older barrels that are never fully emptied. The result was a very sweet and deliciously aged dessert wine called Keo St. John Commandaria from Cypress that was very enjoyable to drink when paired with the olive oil cake."
  },
  {
    "objectID": "food_review/2024-03-07-galit/index.html#rating",
    "href": "food_review/2024-03-07-galit/index.html#rating",
    "title": "Galit (⭐)",
    "section": "",
    "text": "Overall score: 4.6 out of 5.0 stars\nWould definitely go back here if I ever have the opportunity! Thanks for reading, and shalom y’all!"
  },
  {
    "objectID": "book_review.html",
    "href": "book_review.html",
    "title": "Book Reviews",
    "section": "",
    "text": "Reserving this page for when I want to do some book reviews.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog_subscribe.html",
    "href": "blog_subscribe.html",
    "title": "Subscribe to bites | brews | paws!",
    "section": "",
    "text": "I would really appreciate it if you could subscribe to my blog.\nI will put a lot of effort into writing high quality content in the future - and you won’t want to miss it!\nThe emails will be kept to a minimum and you’ll only be alerted to new posts once per week at most.\nThanks, Robert\n\n    \n\n    \n  \n    \n    \n\n    \n    \n\n    \n    \n\n    \n\n      \n        \n        \n      \n\n      \n        \n        \n      \n\n      \n\n            \n            \n            \n            \n            \n            \n      \n\n      \n\n      \n        \n        \n         \n        \n        \n      \n\n        \n        \n        \n        \n        \n        \n      \n\n       \n\n        \n        \n        \n        \n        \n        \n        \n       \n\n\n      \n        \n        \n        \n        \n  \n\n\n\n  \n        \n        \n        \n      \n\n\n      \n    \n    \n    \n    \n    \n    \n    \n  \n\n  \n        \n        \n        \n        \n        \n      \n\n      \n        \n        \n        \n        \n        \n      \n\n      \n        \n        \n        \n        \n        \n      \n\n       \n\n        \n        \n        \n        \n       \n\n       \n        \n        \n        \n        \n      \n\n      \n        \n        \n        \n        \n        \n        \n        \n       \n\n    \n\n    \n\n\n      \n\n\n      \n\n      \n      \n\n      \n\n      \n\n\n\n\n\n    \n\n      \n    \n      \n        \n\n          \n          \n\n          \n\n            \n              \n                Subscribe and receive notification of new blog posts weekly!\n              \n            \n\n            \n              \n\n              \n                \n                  \n                    \n                      \n                      \n\n\n\n\n\n\n                        \n                          \n                          \n                      \n                      \n                        \n\n\n\n                      \n                    \n                  \n\n\n                  \n                    \n                      Subscribe\n                    \n                    \n                      \n                      Loading...\n                    \n                  \n                \n              \n\n              \n              \n              \n\n              \n\n              \n\n              \n\n\n\n\n\n\n              \n              \n\n              \n\n              \n                Subscribe\n                \n                  \n                  Loading...\n                \n              \n              \n            \n          \n\n          \n\n            \n              \n                Thank you!\n                \n                  You have successfully joined our subscriber list."
  },
  {
    "objectID": "about_me.html",
    "href": "about_me.html",
    "title": "About Me",
    "section": "",
    "text": "I’m Robert, an actuarial consultant living in Chicago. I like photography, eating and reviewing good food, travelling, biking, chess, video games, predictive modelling, data visualization, watching Texas Longhorns football, and chilling with my cats. I’m planning on using this blog to write about those things and more, so stay tuned!"
  },
  {
    "objectID": "chess_analysis.html",
    "href": "chess_analysis.html",
    "title": "Chess Analysis",
    "section": "",
    "text": "Reserving this page for when I want to do some analysis of my own chess games. One of my goals for 2024 is to try to get an official USCF chess rating."
  },
  {
    "objectID": "food_review/2024-03-09-sawada-coffee/index.html",
    "href": "food_review/2024-03-09-sawada-coffee/index.html",
    "title": "Sawada Coffee",
    "section": "",
    "text": "Sawada Coffee is an busy and trendy coffee shop in the West Loop neighborhood.\nThey have a small menu, but the drinks (Military Latte, Black Camo Latte, High Five Mocha) are immediately enticing once you step inside and see the cozy shop interior for yourself.\n\n\n\nFirst thing to mention in my review - this place is extremely popular and with good reason.\nIt’s in the busy West Loop area, and while other restaurants are more open and visible especially on a weekend, this cafe is more of a tucked-away gem.\nOnce you step inside the front door, you are greeted by a waiting area with plenty of graffiti and stickers on the walls and a dimly lit chandelier.\n All three of the times I’ve been here, the line was nearly out the door, but fortunately it moves efficiently.\nOnce you move up a few places in line you see it’s a trendy and artsy coffee bar with some unique drink options, and the drinks and decor definitely reflect that.\n\nThe shop is owned by world-renowed (or at least coffee world-renowned…) Hiroshi Sawada, and in 2015 it received recognition as one of the “Best Coffee Shops in Chicago” as well as one of “10 Cool Indie Coffee Shops from Coast to Coast”.\nMy favorite part would have to be the string lights hanging from the ceiling, large tables that almost seem to encourage you to strike up a morning conversation with other customers, and a large sunken dining area with a Texas style BBQ restaurant providing delicious food.\nWhen visiting, it was also so busy that it it was hard to take pictures of the decor without also taking pictures of other customers, which I don’t normally like to do. So honestly, I didn’t worry about taking too many pictures and instead enjoyed the experience. The overall vibe is something that’s best experienced by visiting!\n\n\n\nThe High Five Mocha is a unique and specialty drink. It’s inspired by their sister restaurant, High Five Ramen. Maybe I should do a review of this place in the future?\nMocha is definitely my go-to coffee, and I remember the first time I tried the High Five it tasted nothing like a normal mocha. The first flavor note is actually orange peel, and it comes across really strong.\nThat’s because it has sanshō and orange peel added. Sanshō is an intense, aromatic, high toned Japanese spice. I couldn’t tell you exactly what that tastes like because the High Five Mocha tastes strongly like orange, chocolate, and coffee to me, but I’m sure if I get the chance to try an isolated sample of sanshō in the future it would probably be distinct and recognizable.\nEither way, I drank my coffee way too quickly - 30 seconds flat - and tried a bite of the chocolate cookie.\n\nThe cookie was delicious, of course. I don’t know that I can honestly give a thorough review of a cookie, because it’s just chocolate and powdered sugar. All I can say about it was it was extremely sweet and too much to finish in one serving especially since I had just eaten lunch.\nI was also inspired by my Galit Review to get Lion’s Mane mushroom added to my coffee for an extra $3. Normally I get CBD in my coffee whenever it’s an option, but I’ve always heard of Lion’s Mane being a nootropic that supposedly boosts cognitive performance and memory. I’m not sure if I 100% believe all of the claimed effects, but I do know that mushrooms are healthy for you anyway, so maybe I’ll see some marginal effects!\n\n\n\nOverall score: 4.4 out of 5.0 stars\nOverall, Sawada Coffee is an amazing coffee shop that gets pretty crowded during peak hours. The inside of the cafe is beautiful, you could really spend all day studying here and having pretty unique coffee flavors that you can’t find anywhere else. I love that you can put CBD or Lion’s Mane in your drink for a reasonable charge (although the coffee is specialty and the price reflects that.) And although I didn’t eat it on this visit, the Texas style BBQ served by Green Street is delicious."
  },
  {
    "objectID": "food_review/2024-03-09-sawada-coffee/index.html#trendy-curated",
    "href": "food_review/2024-03-09-sawada-coffee/index.html#trendy-curated",
    "title": "Sawada Coffee",
    "section": "",
    "text": "First thing to mention in my review - this place is extremely popular and with good reason.\nIt’s in the busy West Loop area, and while other restaurants are more open and visible especially on a weekend, this cafe is more of a tucked-away gem.\nOnce you step inside the front door, you are greeted by a waiting area with plenty of graffiti and stickers on the walls and a dimly lit chandelier.\n All three of the times I’ve been here, the line was nearly out the door, but fortunately it moves efficiently.\nOnce you move up a few places in line you see it’s a trendy and artsy coffee bar with some unique drink options, and the drinks and decor definitely reflect that.\n\nThe shop is owned by world-renowed (or at least coffee world-renowned…) Hiroshi Sawada, and in 2015 it received recognition as one of the “Best Coffee Shops in Chicago” as well as one of “10 Cool Indie Coffee Shops from Coast to Coast”.\nMy favorite part would have to be the string lights hanging from the ceiling, large tables that almost seem to encourage you to strike up a morning conversation with other customers, and a large sunken dining area with a Texas style BBQ restaurant providing delicious food.\nWhen visiting, it was also so busy that it it was hard to take pictures of the decor without also taking pictures of other customers, which I don’t normally like to do. So honestly, I didn’t worry about taking too many pictures and instead enjoyed the experience. The overall vibe is something that’s best experienced by visiting!"
  },
  {
    "objectID": "food_review/2024-03-09-sawada-coffee/index.html#high-five-mocha-a-cookie",
    "href": "food_review/2024-03-09-sawada-coffee/index.html#high-five-mocha-a-cookie",
    "title": "Sawada Coffee",
    "section": "",
    "text": "The High Five Mocha is a unique and specialty drink. It’s inspired by their sister restaurant, High Five Ramen. Maybe I should do a review of this place in the future?\nMocha is definitely my go-to coffee, and I remember the first time I tried the High Five it tasted nothing like a normal mocha. The first flavor note is actually orange peel, and it comes across really strong.\nThat’s because it has sanshō and orange peel added. Sanshō is an intense, aromatic, high toned Japanese spice. I couldn’t tell you exactly what that tastes like because the High Five Mocha tastes strongly like orange, chocolate, and coffee to me, but I’m sure if I get the chance to try an isolated sample of sanshō in the future it would probably be distinct and recognizable.\nEither way, I drank my coffee way too quickly - 30 seconds flat - and tried a bite of the chocolate cookie.\n\nThe cookie was delicious, of course. I don’t know that I can honestly give a thorough review of a cookie, because it’s just chocolate and powdered sugar. All I can say about it was it was extremely sweet and too much to finish in one serving especially since I had just eaten lunch.\nI was also inspired by my Galit Review to get Lion’s Mane mushroom added to my coffee for an extra $3. Normally I get CBD in my coffee whenever it’s an option, but I’ve always heard of Lion’s Mane being a nootropic that supposedly boosts cognitive performance and memory. I’m not sure if I 100% believe all of the claimed effects, but I do know that mushrooms are healthy for you anyway, so maybe I’ll see some marginal effects!"
  },
  {
    "objectID": "food_review/2024-03-09-sawada-coffee/index.html#rating",
    "href": "food_review/2024-03-09-sawada-coffee/index.html#rating",
    "title": "Sawada Coffee",
    "section": "",
    "text": "Overall score: 4.4 out of 5.0 stars\nOverall, Sawada Coffee is an amazing coffee shop that gets pretty crowded during peak hours. The inside of the cafe is beautiful, you could really spend all day studying here and having pretty unique coffee flavors that you can’t find anywhere else. I love that you can put CBD or Lion’s Mane in your drink for a reasonable charge (although the coffee is specialty and the price reflects that.) And although I didn’t eat it on this visit, the Texas style BBQ served by Green Street is delicious."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "bites | brews | paws",
    "section": "",
    "text": "This is a Quarto website hosted for free using a GitHub Page.\nTo learn more about hosting Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "photography.html",
    "href": "photography.html",
    "title": "Photography",
    "section": "",
    "text": "Two Weeks in Vietnam\nKedzie Wells L Train S Curve\nFlowers of Đà Lạt\n\n\n\n\n\n\n\n\n\n\nQuincy\n\n\n\n\n\n\n\nEnyo\n\n\n\n\n\n\n\n\nQuincy\nEnyo"
  },
  {
    "objectID": "professional_blog/2024-03-09-spaceship-titanic-part-2/index.html#starting-with-stan",
    "href": "professional_blog/2024-03-09-spaceship-titanic-part-2/index.html#starting-with-stan",
    "title": "Spaceship Titanic - Introduction to Bayesian Models - Part 2",
    "section": "Starting with Stan",
    "text": "Starting with Stan\nStan is a probabilistic programming language for statistical inference. To write a Stan model, I will create a .stan file using the Stan programming language, and then compile and run this model in R using the rstan package.\nI will first show the basic Stan code for the model, and explain each of the blocks:\n\n\n\n\ndata\n\nThis section declares all of the data that will be used in the model.\nThis is data that needs to be provided by the user to run a Stan model.\nI will be using an N x K design matrix that has N distinct observations, and K predictive variables.\n\n\n\nparameters\n\nThis section defines all of the unknown parameters in your model that Stan will estimate using the data.\nIn our model, we will be estimating the \\(\\beta\\) coefficients which are the corresponding linear effects of each of the K predictor variables.\n\n\n\nmodel\n\nThis section will define the statistical model (using a log-likelihood function), prior distributions, and provide the mathematical link between your parameters and observed data.\nIn Part 1 I decided to use a logistic regression. This distribution is implemented in Stan using the bernoulli_logit(...) function.\nNote that Age is a continuous variable, so unless we make any modification to the predictive variable or the model structure, it will be modeled as a linear effect which we saw previously may not be completely accurate. We can still run the model without making any changes and see what the output tells us.\n\n\n\ngenerated quantities\n\nAfter running a Stan model and having some fitted posterior distributions from the data, you may want to sample from these distributions (or even from some algebraically manipulated variation of the distributions). Sampling can be set up in the generated quantities block."
  },
  {
    "objectID": "professional_blog/2024-03-09-spaceship-titanic-part-2/index.html#handling-missing-values",
    "href": "professional_blog/2024-03-09-spaceship-titanic-part-2/index.html#handling-missing-values",
    "title": "Spaceship Titanic - Introduction to Bayesian Models - Part 2",
    "section": "Handling Missing Values",
    "text": "Handling Missing Values\nIt turns out there are some missing values in the Age, VIP, and HomePlanet fields.\nThe easiest way to handle missing values is to impute them using the mice package in R.\nImputation helps us fill in these gaps with plausible values, ensuring that our analysis can proceed without dropping entire rows or columns which might lead to loss of valuable information.\nmice stands for Multivariate Imputation by Chained Equations, a technique that provides a more nuanced approach than simply filling in the missing entries with average values. You could think of it as an iterative regression on the NA values using the non-NA values for that observation.\nI want to make sure I don’t include the Transported field when doing imputation for two reasons:\n\nI don’t have this field available in my test dataset, where I will need to do the same thing.\n\n\n\nThat would be considered a form of [data leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning)).\n\n\nsst_transported &lt;- sst_data_stan$Transported\nsst_no_transported &lt;- sst_data_stan[, -\"Transported\"]\n\nimputed_sst &lt;- mice(sst_no_transported, m = 1, printFlag = FALSE)\nimputed_sst_data &lt;- complete(imputed_sst, 1)\nimputed_sst_data$Transported &lt;- sst_transported\nimputed_sst_data &lt;-\n  imputed_sst_data %&gt;% \n  mutate(HomePlanet = factor(HomePlanet, levels = unique(HomePlanet)),\n         VIP = factor(VIP, levels = unique(VIP)))"
  },
  {
    "objectID": "professional_blog/2024-03-09-spaceship-titanic-part-2/index.html#fitting-the-initial-model",
    "href": "professional_blog/2024-03-09-spaceship-titanic-part-2/index.html#fitting-the-initial-model",
    "title": "Spaceship Titanic - Introduction to Bayesian Models - Part 2",
    "section": "Fitting the Initial Model",
    "text": "Fitting the Initial Model\nHere is what the first 10 rows of the design matrix (denoted as X in our Stan code) looks like:\n\nimputed_sst_data %&gt;% \n  model.matrix(`Transported` ~ `Age` + `VIP` + `HomePlanet`, data = .) %&gt;% \n  as.data.frame() %&gt;% \n  slice(1:10) %&gt;% \n  rj_custom_table()\n\n(Intercept)AgeVIPTRUEHomePlanetEarthHomePlanetMars139000124010158100133000116010144010126010128010135010114000\n\n\nFrom the design matrix and the Stan code we can see that this model will need to estimate five separate \\(\\beta\\) parameters, one for each column of the design matrix.\nI will now fit the initial model using the stan(...) function from the rstan library.\nReferencing the parameters I defined in the data block in the Stan code above, we will need to provide the stan(...) function with a named list of parameters N, K, X, and y. Note that I am running a model with 2 chains of 2,000 iterations each (which will be split into 1,000 warm-up iterations and 1,000 sampling iterations per chain.)\nYou can think of the model fitting as exploring a potentially jagged K-dimensional terrain across the parameter space and the log-likelihood of observing the data at each point using that specific set of parameters is the height. We start in 4 random locations on this terrain, and hope that these chains end up in the exact same spot of maximum likelihood and stabilize so that we can draw credible and usable posterior parameter estimates.\nThe exact method that Stan uses to explore this terrain is complex and definitely beyond the scope of this blog, but if you’re interested in learning more, you should read about Hamiltonian Monte Carlo and the no-U-turn sampler.\n\nsst_design_matrix &lt;- \n  imputed_sst_data %&gt;% \n  model.matrix(`Transported` ~ `Age` + `VIP` + `HomePlanet`, data = .)\n\nsst_response &lt;- as.integer(imputed_sst_data$Transported)\n\nif (!file.exists(\"../../assets/models/sst_model_1.rds\")){\n  sst_fit_1 &lt;- \n    stan(file = \"../../assets/stan/spaceship_titanic_2.stan\",\n         data = list(N = nrow(sst_design_matrix),\n                     K = ncol(sst_design_matrix),\n                     X = sst_design_matrix,\n                     y = sst_response),\n         chains = 2,\n         iter = 2000)\n  save(sst_fit_1, file = \"../../assets/models/sst_model_1.rds\")\n  } else {\n    load(\"../../assets/models/sst_model_1.rds\")\n  }"
  },
  {
    "objectID": "professional_blog/2024-03-09-spaceship-titanic-part-2/index.html#model-diagnostics",
    "href": "professional_blog/2024-03-09-spaceship-titanic-part-2/index.html#model-diagnostics",
    "title": "Spaceship Titanic - Introduction to Bayesian Models - Part 2",
    "section": "Model Diagnostics",
    "text": "Model Diagnostics\nThat model was easy enough to set up and run! Let’s examine what the resulting posterior distributions look like.\n\nmcmc_hist(as.array(sst_fit_1, pars = \"beta\"),\n          facet_args = list(labeller = ggplot2::label_parsed)) +\n  theme(text = element_text(size = 24),\n        axis.text = element_text(size = 18), # Tick text size\n        axis.title = element_text(size = 24), # Axis label size\n        strip.text = element_text(size = 24))\n\n\n\n\n\n\n\n\nInterestingly, it looks like our posterior distributions for the \\(\\beta\\) parameters are looking approximately normal with reasonably tight standard errors, even though we didn’t even set any explicit prior distributions at all in the model block of the Stan code. Normally, the advantage of Bayesian models is the ability to incorporate expert opinion through prior distributions but this is not required. Instead you can use what is called a flat prior which basically indicates we have no prior information about what the distribution or mean of the \\(\\beta\\) parameters would be.\n\nrounded_betas &lt;- rstan::extract(sst_fit_1)$beta %&gt;% \n  apply(2, mean) %&gt;% \n  round(4)\n\nnames(rounded_betas) &lt;- c(\"beta[1]\", \"beta[2]\", \"beta[3]\", \"beta[4]\", \"beta[5]\")\n\nrounded_betas %&gt;%\n  t() %&gt;% \n  as.data.frame() %&gt;% \n  rj_custom_table()\n\nbeta[1]beta[2]beta[3]beta[4]beta[5]1.3045-0.0173-0.8962-1.163-0.6726\n\n\nOne final check: Are these \\(\\beta\\) means in the same direction of what we expected from the univariate analysis in Part 1?\n\nbeta[1]: (Intercept)\n\nThis quantity represents the baseline, which should correspond to a passenger who is Age 0, not a VIP, and having a home planet of Europa.\nTo convert into a probability, we can undo the logit function manually.\n\\(p = \\frac{e^{\\beta_1}}{1 + e^{\\beta_1}}\\)\n\\(p = 78.7\\%\\)\n\n\n\nbeta[2]: (Age)\n\nWe can see a decreasing effect in Age, where every increase of Age by 1 year decreases the log odds of Transportation by -0.0173.\nWe did observe a decreasing effect with Age although it wasn’t exactly linear and mostly flat after Age 17.\n\n\n\nbeta[3]: (VIP = TRUE)\n\nBeing a VIP reduces the log odds of Transportation by -0.8962 compared to the baseline of not being a VIP.\nWe can demonstrate what that means in terms of probability by calculating the Transportation prediction to a passenger who is Age 0, a VIP, and having a home planet of Europa and comparing that to the baseline probability of 78.7%.\n\\(p = \\frac{e^{\\beta1 + \\beta3}}{1 + e^{\\beta1 + \\beta3}}\\)\n\\(p = 63.3\\%\\)\n\n\n\nbeta[4]: (Home Planet = Earth)\n\nHaving a home planet of Earth reduces the log odds of Transportation by -1.1629 compared to the baseline of Europa.\n\n\n\nbeta[5]: (Home Planet = Mars)\n\nHaving a home planet of Mars reduces the log odds of Transportation by -0.6846 compared to the baseline of Europa.\n\nAnd finally, how do the new model’s predictions perform on the training data? Remember that roughly 50% of the travelers were Transported in the original data, so let’s see if we can identify who was Transported more accurately than randomly guessing by using a confusion matrix.\nSince we are testing the model on the same data we trained it on, these metrics are also referred to as training error and probably an optimistic estimate of how the model would generalize to data it hasn’t been trained on before.\n\ny_pred_means &lt;- apply(rstan::extract(sst_fit_1, \"y_pred\")$y_pred, 2, mean)\ny_pred_factor &lt;- factor(ifelse(y_pred_means &gt; 0.5, TRUE, FALSE))\nsst_response_factor &lt;- factor(ifelse(sst_response == 1, TRUE, FALSE))\n\nconfusionMatrix(y_pred_factor, sst_response_factor)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction FALSE TRUE\n     FALSE  2884 2041\n     TRUE   1431 2337\n                                          \n               Accuracy : 0.6006          \n                 95% CI : (0.5902, 0.6109)\n    No Information Rate : 0.5036          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.202           \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n                                          \n            Sensitivity : 0.6684          \n            Specificity : 0.5338          \n         Pos Pred Value : 0.5856          \n         Neg Pred Value : 0.6202          \n             Prevalence : 0.4964          \n         Detection Rate : 0.3318          \n   Detection Prevalence : 0.5665          \n      Balanced Accuracy : 0.6011          \n                                          \n       'Positive' Class : FALSE           \n                                          \n\n\nAccuracy is one of the most straightforward metrics derived from a confusion matrix. It represents the proportion of true results (both true positives and true negatives) among the total number of cases examined. With no information, we saw that we could randomly guess Transportation correctly 50.4% of the time. Now, given a random traveler we can predict if they were transported or not correctly 60.1% of the time."
  },
  {
    "objectID": "professional_blog/2024-03-09-spaceship-titanic-part-2/index.html#summary-part-3-preview",
    "href": "professional_blog/2024-03-09-spaceship-titanic-part-2/index.html#summary-part-3-preview",
    "title": "Spaceship Titanic - Introduction to Bayesian Models - Part 2",
    "section": "Summary + Part 3 Preview",
    "text": "Summary + Part 3 Preview\nI’d say those are some promising results considering this is the first iteration of the model. I only included three basic factors without too much data exploration, and was able to improve the accuracy from 50.4% to 60.1%. I’d like to see if we can get our training accuracy to at least 80% by the end of this series, which may or may not be possible given the complexity of the data.\nThere are a couple of specific improvements I plan on making in Part 3:\n\nComparing a new model to this one to test for improvement using an ROC curve.\nFitting Age using a non-linear fitted effect instead of a linear effect term (possibly with the help of a generalized additive model or spline.)\nExploring the use of Cabin as a predictor variable, and eventually using a hierarchical model (also called multilevel model) structure.\n\nStay tuned for Part 3, and thanks again for reading Part 2 of the Spaceship Titanic series using Bayesian modeling.\nUPDATE: Part 3 is out! You can read it here.\n\nReturn to the top!"
  },
  {
    "objectID": "professional_blog/2024-03-11-march-madness-1/index.html#loading-data",
    "href": "professional_blog/2024-03-11-march-madness-1/index.html#loading-data",
    "title": "A Descent into March Madness",
    "section": "Loading Data",
    "text": "Loading Data\nI have always wanted to build a March Madness model, so I will be using the March Machine Learning Mania data set on Kaggle to build a predictive model and use it to fill out a bracket once the tournament seeds for 2024 are finalized. This competition has a lot of available data sets provided, including the final scores and stats of every single college basketball game since the 1984-1985 season. That’s intense, and too many fields to work with effective with just a week to go until the tournament. That’s why I’ll be keeping things basic again and starting off with a couple of predictor variables: Seed, Win Ratio, and Average Margin of Victory/Loss.\nI will also be focusing more on the (hopefully very) predictive aspect and less on the interpretability of this model.\n\nlibrary(data.table)\nlibrary(rstan)\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(tidyr)\n\nbb_teams &lt;- fread(\"C:/Users/rober/OneDrive/Desktop/data/mm_data/MTeams.csv\")\nbb_seasons &lt;- fread(\"C:/Users/rober/OneDrive/Desktop/data/mm_data/MSeasons.csv\")\nbb_data &lt;- fread(\"C:/Users/rober/OneDrive/Desktop/data/mm_data/MRegularSeasonCompactResults.csv\")\nbb_rankings &lt;- fread(\"C:/Users/rober/OneDrive/Desktop/data/mm_data/MMasseyOrdinals.csv\")\nbb_seeds &lt;- fread(\"C:/Users/rober/OneDrive/Desktop/data/mm_data/MNCAATourneySeeds.csv\")\nbb_tourney &lt;- fread(\"C:/Users/rober/OneDrive/Desktop/data/mm_data/MNCAATourneyCompactResults.csv\")\nbb_sample &lt;- fread(\"C:/Users/rober/OneDrive/Desktop/data/mm_data/SampleSubmission2023.csv\")\n\nOnce I’ve loaded in the various data files, my strategy to set up my first few predictors will be to closely follow this Kaggle Notebook to come up with a base structure that I can improve on later.\n\nbb_data %&lt;&gt;% mutate(ScoreGap = WScore - LScore)\n\n# Number of Wins\nnum_win &lt;- bb_data %&gt;%\n  group_by(Season, WTeamID) %&gt;%\n  summarise(NumWins = n()) %&gt;%\n  rename(TeamID = WTeamID)\n\n# Number of Losses\nnum_loss &lt;- bb_data %&gt;%\n  group_by(Season, LTeamID) %&gt;%\n  summarise(NumLosses = n()) %&gt;%\n  rename(TeamID = LTeamID)\n\n# Average Score Gap for Wins\ngap_win &lt;- bb_data %&gt;%\n  group_by(Season, WTeamID) %&gt;%\n  summarise(GapWins = mean(ScoreGap, na.rm = TRUE)) %&gt;%\n  rename(TeamID = WTeamID)\n\n# Average Score Gap for Losses\ngap_loss &lt;- bb_data %&gt;%\n  group_by(Season, LTeamID) %&gt;%\n  summarise(GapLosses = mean(ScoreGap, na.rm = TRUE)) %&gt;%\n  rename(TeamID = LTeamID)\n\n# Features for each season\nbb_features_w &lt;- \n  bb_data %&gt;%\n  select(Season, WTeamID) %&gt;% \n  unique() %&gt;% \n  rename(TeamID = WTeamID)\n\nbb_features_l &lt;- \n  bb_data %&gt;%\n  select(Season, LTeamID) %&gt;% \n  unique() %&gt;% \n  rename(TeamID = LTeamID)\n\nbb_features &lt;- \n  bind_rows(bb_features_w, bb_features_l) %&gt;% \n  distinct() %&gt;% \n  arrange(Season, TeamID)\n\n# Merge features df with num_win, num_loss, gap_win, gap_loss.\nbb_features %&lt;&gt;% \n  merge(num_win, by = c(\"Season\", \"TeamID\"), all.x = TRUE) %&gt;% \n  merge(num_loss, by = c(\"Season\", \"TeamID\"), all.x = TRUE) %&gt;% \n  merge(gap_win, by = c(\"Season\", \"TeamID\"), all.x = TRUE) %&gt;% \n  merge(gap_loss, by = c(\"Season\", \"TeamID\"), all.x = TRUE)\n\nbb_features %&lt;&gt;% replace(is.na(.), 0)\n\nbb_features %&lt;&gt;% \n  mutate(WinRatio = NumWins / (NumWins + NumLosses),\n         GapAvg = (NumWins * GapWins - NumLosses * GapLosses) / (NumWins + NumLosses)) %&gt;% \n  select(-NumWins, -NumLosses, -GapWins, -GapLosses)\n\nIn the code above I created variables called WinRatio and GapAvg using the historical regular season results for every team and every season going back to 1985. Now I will need to take the historical tournament results and merge the features we just created with that."
  },
  {
    "objectID": "professional_blog/2024-03-11-march-madness-1/index.html#historical-tournament-results",
    "href": "professional_blog/2024-03-11-march-madness-1/index.html#historical-tournament-results",
    "title": "A Descent into March Madness",
    "section": "Historical Tournament Results",
    "text": "Historical Tournament Results\n\nbb_tourney %&lt;&gt;% \n  merge(bb_seeds, by.x = c(\"Season\", \"WTeamID\"), by.y = c(\"Season\", \"TeamID\"), all.x = TRUE) %&gt;% \n  rename(\"SeedW\" = \"Seed\") %&gt;% \n  merge(bb_seeds, by.x = c(\"Season\", \"LTeamID\"), by.y = c(\"Season\", \"TeamID\"), all.x = TRUE) %&gt;% \n  rename(\"SeedL\" = \"Seed\") %&gt;% \n  mutate(SeedW = as.integer(gsub(\"\\\\D\", \"\", SeedW)), SeedL = as.integer(gsub(\"\\\\D\", \"\", SeedL))) %&gt;% \n  merge(bb_features, by.x = c(\"Season\", \"WTeamID\"), by.y = c(\"Season\", \"TeamID\"), all.x = TRUE) %&gt;%   \n  rename(\"GapAvgW\" = \"GapAvg\", \"WinRatioW\" = \"WinRatio\") %&gt;% \n  merge(bb_features, by.x = c(\"Season\", \"LTeamID\"), by.y = c(\"Season\", \"TeamID\"), all.x = TRUE) %&gt;% \n  rename(\"GapAvgL\" = \"GapAvg\", \"WinRatioL\" = \"WinRatio\") %&gt;% \n  select(-NumOT, -WLoc)\n\n# Adding a reversed version of the data to itself so we have both wins and losses.\n\nbb_1 &lt;- \n  bb_tourney %&gt;% \n  rename(\"TeamIdA\" = \"WTeamID\",\n         \"ScoreA\" = \"WScore\",\n         \"TeamIdB\" = \"LTeamID\",\n         \"ScoreB\" = \"LScore\",\n         \"SeedA\" = \"SeedW\",\n         \"SeedB\" = \"SeedL\",\n         \"WinRatioA\" = \"WinRatioW\",\n         \"WinRatioB\" = \"WinRatioL\",\n         \"GapAvgA\" = \"GapAvgW\",\n         \"GapAvgB\" = \"GapAvgL\")\n\nbb_2 &lt;-\n  bb_tourney %&gt;% \n  rename(\"TeamIdB\" = \"WTeamID\",\n         \"ScoreB\" = \"WScore\",\n         \"TeamIdA\" = \"LTeamID\",\n         \"ScoreA\" = \"LScore\",\n         \"SeedB\" = \"SeedW\",\n         \"SeedA\" = \"SeedL\",\n         \"WinRatioB\" = \"WinRatioW\",\n         \"WinRatioA\" = \"WinRatioL\",\n         \"GapAvgB\" = \"GapAvgW\",\n         \"GapAvgA\" = \"GapAvgL\")\n\nbb &lt;- rbind(bb_1, bb_2 %&gt;% relocate(names(bb_1)))\n  \nbb %&lt;&gt;% mutate(SeedDiff = SeedA - SeedB,\n               WinRatioDiff = WinRatioA - WinRatioB,\n               GapAvgDiff = GapAvgA - GapAvgB,\n               ScoreDiff = ScoreA - ScoreB,\n               WinA = as.integer(ScoreDiff &gt; 0))\n\nbb_train &lt;-\n  bb %&gt;% \n  select(\"SeedA\", \"SeedB\", \"WinRatioA\", \"WinRatioB\", \"GapAvgA\", \"GapAvgB\", \"SeedDiff\", \"WinRatioDiff\", \"GapAvgDiff\", \"WinA\") %&gt;% \n  mutate(across(SeedA:GapAvgDiff, scale))\n\nrm(list = setdiff(ls(), c(\"bb_train\", names(sessionInfo()$otherPkgs))))\ngc()\n\n          used (Mb) gc trigger  (Mb) max used  (Mb)\nNcells 1233903 65.9    2064454 110.3  2064454 110.3\nVcells 2243856 17.2   25132175 191.8 23883274 182.3\n\n\nJust for the heck of it, let’s create an XGBoost model. For this competition I think it might be fun to create two or three independent models, and then stack the predictions together for one final prediction.\n\nlibrary(xgboost)\nlibrary(pROC)\n\nX &lt;- as.matrix(bb_train %&gt;% select(-WinA))\ny &lt;- bb_train$WinA\n\ndtrain &lt;- xgb.DMatrix(data = X, label = y)\n\nparams &lt;- list(\n  objective = \"binary:logistic\",\n  eta = 0.3,\n  max_depth = 6,\n  eval_metric = \"logloss\"\n)\n\nnrounds &lt;- 100\n\nm1 &lt;- xgb.train(params = params, data = dtrain, nround = nrounds)\np1 &lt;- predict(m1, dtrain)\np1_binary &lt;- ifelse(p1 &gt; 0.5, 1, 0)\n\naccuracy &lt;- mean(p1_binary == bb_train$WinA)\nprint(paste0(\"Accuracy: \", accuracy))\n\n[1] \"Accuracy: 0.953896368829049\"\n\n\nIt seems like the accuracy of the model is extremely high - probably due to some type of data leakage. I’ll keep the model structure how it is for now and see how the predictions turn out."
  },
  {
    "objectID": "professional_blog/2024-03-11-march-madness-1/index.html#summary",
    "href": "professional_blog/2024-03-11-march-madness-1/index.html#summary",
    "title": "A Descent into March Madness",
    "section": "Summary",
    "text": "Summary\nWorking with sports data is difficult and tedious. It’s also very easy to introduce data leakage into a temporal model, and it’s hard to evaluate if a prediction accuracy that high is actually possible. I have one week to look over the code for bugs and improve the model though, so hopefully if there is a problem I will catch it.\nI could run the same model on the 2023 Tournament Data and submit and see how it fares, because I doubt that 95% accuracy is possible.\nThere are also so many possible predictor variables and the variability of who wins a single basketball game is extremely high. If it was easy to model, there would have been many perfect brackets by now (and millionaire data scientists who are taking advantage of casinos with their high accuracy model.)\nIn this post, I started the model structure using an existing Kaggle Notebook and fit an XGBoost model to a few NCAA season team statistics like SeedDiff, WinRatioDiff, and GapAvgDiff. Before the week is over, I would like to test the model on 2023 competition and also fit one or two more models to be able to stack predictions, so consider this blog a work in progress.\nThanks for reading, and please consider subscribing.\n\nReturn to the top!"
  }
]